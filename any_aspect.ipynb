{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "any_aspect.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMpmOGq7ggCk",
        "outputId": "185b406c-ebab-43da-b602-de81dce5a0f5"
      },
      "source": [
        "%cd /content/drive/MyDrive/상영/aspect-based-summarization-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/상영/aspect-based-summarization-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYW2IEejb64j",
        "outputId": "48b68e86-172a-4f3f-c08f-3c9ae36998fa"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fire==0.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.3.1)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.7.1)\n",
            "Requirement already satisfied: transformers==4.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.3.3)\n",
            "Requirement already satisfied: pytorch-lightning==1.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: torchtext==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: fairseq==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: loguru==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: cleantext==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.1.3)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.5.2)\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.5.0)\n",
            "Requirement already satisfied: kobart from git+https://github.com/SKT-AI/KoBART#egg=kobart in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire==0.3.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire==0.3.1->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (0.18.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (5.3.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (2021.4.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.0->-r requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.0->-r requirements.txt (line 6)) (1.0.6)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.0->-r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.0->-r requirements.txt (line 6)) (0.29.22)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.0->-r requirements.txt (line 6)) (0.6)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.10.0->-r requirements.txt (line 6)) (1.14.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from cleantext==1.1.3->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy->-r requirements.txt (line 9)) (3.10.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy->-r requirements.txt (line 9)) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy->-r requirements.txt (line 9)) (1.2.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy->-r requirements.txt (line 9)) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy->-r requirements.txt (line 9)) (4.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.3->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (1.28.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (3.7.4.post0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.0->-r requirements.txt (line 6)) (5.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.0->-r requirements.txt (line 6)) (4.8)\n",
            "Requirement already satisfied: omegaconf<2.1,>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==0.10.0->-r requirements.txt (line 6)) (2.0.6)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.10.0->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.10.0->-r requirements.txt (line 6)) (2.20)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (20.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy->-r requirements.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2->-r requirements.txt (line 4)) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ3m4XMFzieY"
      },
      "source": [
        "!python construct.py --split train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZrwfkiI38O7",
        "outputId": "2c3faac0-c1a7-4d09-8ce9-2fa2db7ba5d9"
      },
      "source": [
        "!python finetune.py --dataset_name earphone --train_docs 61000 --n_epochs 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n",
            "\u001b[32m2021-05-08 09:59:27.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mpretrained checkpoint: None\u001b[0m\n",
            "using cached model\n",
            "using cached model\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "2021-05-08 09:59:37.664962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\n",
            "  | Name   | Type                         | Params\n",
            "--------------------------------------------------------\n",
            "0 | _model | BartForConditionalGeneration | 123 M \n",
            "--------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]\u001b[32m2021-05-08 09:59:40.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.bart\u001b[0m:\u001b[36mvalidation_epoch_end\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mepoch 0, lr = 0.0, val loss = 11.587813377380371\u001b[0m\n",
            "Epoch 0:   0% 0/137479 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Epoch 0:  98% 134800/137479 [2:42:38<03:13, 13.81it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/2695 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  98% 134820/137479 [2:42:39<03:12, 13.81it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134840/137479 [2:42:40<03:11, 13.81it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134860/137479 [2:42:41<03:09, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134880/137479 [2:42:42<03:08, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134900/137479 [2:42:42<03:06, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134920/137479 [2:42:43<03:05, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134940/137479 [2:42:44<03:03, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134960/137479 [2:42:45<03:02, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 134980/137479 [2:42:46<03:00, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135000/137479 [2:42:46<02:59, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135020/137479 [2:42:47<02:57, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135040/137479 [2:42:48<02:56, 13.82it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135060/137479 [2:42:49<02:54, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135080/137479 [2:42:50<02:53, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135100/137479 [2:42:50<02:52, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135120/137479 [2:42:51<02:50, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135140/137479 [2:42:52<02:49, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135160/137479 [2:42:53<02:47, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135180/137479 [2:42:53<02:46, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135200/137479 [2:42:54<02:44, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135220/137479 [2:42:55<02:43, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135240/137479 [2:42:56<02:41, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135260/137479 [2:42:57<02:40, 13.83it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135280/137479 [2:42:57<02:38, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135300/137479 [2:42:58<02:37, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135320/137479 [2:42:59<02:36, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135340/137479 [2:43:00<02:34, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135360/137479 [2:43:01<02:33, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135380/137479 [2:43:01<02:31, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  98% 135400/137479 [2:43:02<02:30, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135420/137479 [2:43:03<02:28, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135440/137479 [2:43:04<02:27, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135460/137479 [2:43:05<02:25, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135480/137479 [2:43:05<02:24, 13.84it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135500/137479 [2:43:06<02:22, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135520/137479 [2:43:07<02:21, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135540/137479 [2:43:08<02:20, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135560/137479 [2:43:09<02:18, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135580/137479 [2:43:10<02:17, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135600/137479 [2:43:10<02:15, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135620/137479 [2:43:11<02:14, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135640/137479 [2:43:12<02:12, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135660/137479 [2:43:13<02:11, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135680/137479 [2:43:14<02:09, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135700/137479 [2:43:14<02:08, 13.85it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135720/137479 [2:43:15<02:06, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135740/137479 [2:43:16<02:05, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135760/137479 [2:43:17<02:04, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135780/137479 [2:43:18<02:02, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135800/137479 [2:43:18<02:01, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135820/137479 [2:43:19<01:59, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135840/137479 [2:43:20<01:58, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135860/137479 [2:43:21<01:56, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135880/137479 [2:43:22<01:55, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135900/137479 [2:43:22<01:53, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135920/137479 [2:43:23<01:52, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135940/137479 [2:43:24<01:50, 13.86it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135960/137479 [2:43:25<01:49, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 135980/137479 [2:43:26<01:48, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136000/137479 [2:43:26<01:46, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136020/137479 [2:43:27<01:45, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136040/137479 [2:43:28<01:43, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136060/137479 [2:43:29<01:42, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136080/137479 [2:43:30<01:40, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136100/137479 [2:43:30<01:39, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136120/137479 [2:43:31<01:37, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136140/137479 [2:43:32<01:36, 13.87it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136160/137479 [2:43:33<01:35, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136180/137479 [2:43:34<01:33, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136200/137479 [2:43:34<01:32, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136220/137479 [2:43:35<01:30, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136240/137479 [2:43:36<01:29, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136260/137479 [2:43:37<01:27, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136280/137479 [2:43:38<01:26, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136300/137479 [2:43:38<01:24, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136320/137479 [2:43:39<01:23, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136340/137479 [2:43:40<01:22, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136360/137479 [2:43:41<01:20, 13.88it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136380/137479 [2:43:41<01:19, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136400/137479 [2:43:42<01:17, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136420/137479 [2:43:43<01:16, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136440/137479 [2:43:44<01:14, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136460/137479 [2:43:45<01:13, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136480/137479 [2:43:45<01:11, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136500/137479 [2:43:46<01:10, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136520/137479 [2:43:47<01:09, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136540/137479 [2:43:48<01:07, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136560/137479 [2:43:48<01:06, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136580/137479 [2:43:49<01:04, 13.89it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136600/137479 [2:43:50<01:03, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136620/137479 [2:43:51<01:01, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136640/137479 [2:43:52<01:00, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136660/137479 [2:43:52<00:58, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136680/137479 [2:43:53<00:57, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136700/137479 [2:43:54<00:56, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136720/137479 [2:43:55<00:54, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136740/137479 [2:43:56<00:53, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136760/137479 [2:43:56<00:51, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0:  99% 136780/137479 [2:43:57<00:50, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136800/137479 [2:43:58<00:48, 13.90it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136820/137479 [2:43:59<00:47, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136840/137479 [2:43:59<00:45, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136860/137479 [2:44:00<00:44, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136880/137479 [2:44:01<00:43, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136900/137479 [2:44:02<00:41, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136920/137479 [2:44:03<00:40, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136940/137479 [2:44:03<00:38, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136960/137479 [2:44:04<00:37, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 136980/137479 [2:44:05<00:35, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137000/137479 [2:44:06<00:34, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137020/137479 [2:44:06<00:32, 13.91it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137040/137479 [2:44:07<00:31, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137060/137479 [2:44:08<00:30, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137080/137479 [2:44:09<00:28, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137100/137479 [2:44:10<00:27, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137120/137479 [2:44:10<00:25, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137140/137479 [2:44:11<00:24, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137160/137479 [2:44:12<00:22, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137180/137479 [2:44:13<00:21, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137200/137479 [2:44:14<00:20, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137220/137479 [2:44:14<00:18, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137240/137479 [2:44:15<00:17, 13.92it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137260/137479 [2:44:16<00:15, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137280/137479 [2:44:17<00:14, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137300/137479 [2:44:18<00:12, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137320/137479 [2:44:18<00:11, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137340/137479 [2:44:19<00:09, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137360/137479 [2:44:20<00:08, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137380/137479 [2:44:21<00:07, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137400/137479 [2:44:21<00:05, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137420/137479 [2:44:22<00:04, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137440/137479 [2:44:23<00:02, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Epoch 0: 100% 137460/137479 [2:44:24<00:01, 13.93it/s, loss=0.00619, v_num=15, train_loss=6.63e-5]\n",
            "Validating:  99% 2680/2695 [01:46<00:00, 25.16it/s]\u001b[A\n",
            "Validating: 100% 2695/2695 [01:46<00:00, 25.16it/s]\u001b[A\u001b[32m2021-05-08 12:44:06.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.bart\u001b[0m:\u001b[36mvalidation_epoch_end\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mepoch 0, lr = 3e-05, val loss = 0.0054708604167384815\u001b[0m\n",
            "Epoch 0: 100% 137479/137479 [2:44:25<00:00, 13.93it/s, loss=0.00571, v_num=15, train_loss=0.000189]\n",
            "                                                   \u001b[AEpoch 0, global step 13478: val_loss reached 0.00547 (best 0.00547), saving model to \"/content/drive/My Drive/상영/aspect-based-summarization-master/logs/earphone/docs61000/best_model/epoch=0-step=13478.ckpt\" as top 1\n",
            "Saving hparams to file_path: /content/drive/My Drive/상영/aspect-based-summarization-master/lightning_logs/version_15/hparams.yaml\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "tcmalloc: large alloc 1231454208 bytes == 0x55e9380a6000 @  0x7fa949eed615 0x55e7cec6fcdc 0x55e7ced4f52a 0x55e7cec7676c 0x7fa937616a94 0x7fa937618864 0x7fa9375e8590 0x7fa927f2e465 0x7fa927f2a9ca 0x7fa927f2f609 0x7fa9375ebf2b 0x7fa937271200 0x55e7cec738a8 0x55e7cece6fd5 0x55e7cece17ad 0x55e7cec743ea 0x55e7cece23b5 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece67f0 0x55e7cec7430a 0x55e7cece23b5 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece260e 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece67f0 0x55e7cec7430a 0x55e7cece260e 0x55e7cec7430a\n",
            "tcmalloc: large alloc 1539317760 bytes == 0x55e8b85ec000 @  0x7fa949eed615 0x55e7cec6fcdc 0x55e7ced4f52a 0x55e7cec7676c 0x7fa937616a94 0x7fa937618864 0x7fa9375e8590 0x7fa927f2e465 0x7fa927f2a9ca 0x7fa927f2f609 0x7fa9375ebf2b 0x7fa937271200 0x55e7cec738a8 0x55e7cece6fd5 0x55e7cece17ad 0x55e7cec743ea 0x55e7cece23b5 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece67f0 0x55e7cec7430a 0x55e7cece23b5 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece260e 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece67f0 0x55e7cec7430a 0x55e7cece260e 0x55e7cec7430a\n",
            "tcmalloc: large alloc 1924153344 bytes == 0x55e9141ee000 @  0x7fa949eed615 0x55e7cec6fcdc 0x55e7ced4f52a 0x55e7cec7676c 0x7fa937616a94 0x7fa937618864 0x7fa9375e8590 0x7fa927f2e465 0x7fa927f2a9ca 0x7fa927f2f609 0x7fa9375ebf2b 0x7fa937271200 0x55e7cec738a8 0x55e7cece6fd5 0x55e7cece17ad 0x55e7cec743ea 0x55e7cece23b5 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece67f0 0x55e7cec7430a 0x55e7cece23b5 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece260e 0x55e7cece14ae 0x55e7cec743ea 0x55e7cece67f0 0x55e7cec7430a 0x55e7cece260e 0x55e7cec7430a\n",
            "Epoch 1:  60% 83060/137479 [1:40:11<1:05:38, 13.82it/s, loss=0.00433, v_num=15, train_loss=0.000374]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZMbJeQ8wT1y",
        "outputId": "5f197791-2e83-4597-e268-504b3738c683"
      },
      "source": [
        "!python get_model_binary.py --hparams logs/earphone/docs10000/best_model/hparams.yaml --model_binary logs/earphone/docs10000/best_model/epoch=0-step=2611.ckpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  hparams = yaml.load(f)\n",
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjUQRYqwRDZ9"
      },
      "source": [
        "!python generate.py --log_path logs/aspect_summary"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}